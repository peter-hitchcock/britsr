---
title: "Key Results"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=14, fig.height=12)
```


# Prep  

```{r, results = hide}
# Load packages, source functions, set plot parameters, get behavioral and DIC dfs   
packages_to_load <- c(
  "dplyr",
  "tidyverse",
  "latex2exp",
  "purrr",
  "tidyr",
  "rlang",
  "patchwork",
  "haven",
  "RWiener",
  "psych",
  "data.table",
  "lme4", 
  "lmerTest",
  "foreach",
  "beset",
  "faux"
)
sapply(packages_to_load, require, character.only=TRUE)

sf <- function() sapply(fs, source)
fs <- c(
  paste0('./Modules/', list.files('./Modules/'))  
)
sf()

SetPlotPars() 
```


```{r}
# Get main SRET_CTL df..
bx_df <- read.csv("./../../data/cleaned_files/s_bdf.csv")
# .. and df with some extra info to put in 
extra_df_full <- read.csv("./../../data/raw_files/tx_cond+ffmq+maas_from_KW_6.24.2020.csv")
```



# Figure 1  

DDM Vis  
```{r}
starting_point <- .05
non_dec_time <- c(.1)
threshold <- 0
decision_bounds <- c(max(non_dec_time), (max(non_dec_time)+1+threshold))
decision_time <- seq(decision_bounds[1], decision_bounds[2],
                     by=(decision_bounds[2]-decision_bounds[1])/100)

CreateDriftTrajectories <- function(upper_drift, lower_drift, n_trajects, label, sd_drift=.02) {
  ### Simulate n_trajects drift trajectories ###
  dfs <- list()
  for (nt in 1:n_trajects) {
    this_upper_drift <- rnorm(1, upper_drift, .01)
    this_lower_drift <- rnorm(1, lower_drift, .01)
      traject <- c()
      for (dec_ts in seq_along(decision_time)) {
        y_incr <- rnorm(1, this_upper_drift, sd=sd_drift) 
        n_incr <- rnorm(1, this_lower_drift, sd=sd_drift) 
        incr_total <- sum(y_incr, n_incr)
        if (dec_ts == 1) traject[dec_ts] <- starting_point
        if (dec_ts > 1) traject[dec_ts] <- traject[dec_ts-1] + incr_total
    }
    traject[traject > 1] <- 1
    traject[traject < -1] <- -1
    dfs[[nt]] <- setNames(data.frame(traject, label, decision_time, nt), 
                          c("trajectory", "drift_type", "step_index", "run"))
  }
dfs %>% bind_rows()
}

drift_1 <- CreateDriftTrajectories(.06, -.01, 12, "high", sd_drift=.02)
drift_2 <- CreateDriftTrajectories(.06, -.03, 12, "medium", sd_drift=.02)
drift_3 <- CreateDriftTrajectories(.03, -.07, 12, "low", sd_drift=.02)


all_drifts <- rbind(drift_1, drift_2, drift_3)
all_drifts$drift_type <- factor(all_drifts$drift_type, levels=c("high", "medium", "low"))

rect <- data.frame(xmin=0, xmax=seq(non_dec_time, .2, .033), ymin=-1, ymax=1)

drift_sample <- ggplot(all_drifts, aes(step_index, trajectory, group=interaction(run, drift_type))) + 
    geom_line(size = 2, alpha=.5, aes(color=drift_type)) + 
    geom_hline(yintercept=c(-seq(1, 1.6, .1), seq(1, 1.6, .1)), 
               alpha=c(rev(seq(.4, 1, .1)), rev(seq(.4, 1, .1))), color="gray57", size=2) +
   geom_segment(aes(x=0, xend=non_dec_time, y=.05, yend=.05),
               inherit.aes = FALSE, color="black", size = 2) +
   geom_segment(aes(x=0, xend=non_dec_time, y=.2, yend=.2),
               color="gray57", size = 2) +
    geom_segment(aes(x=0, xend=non_dec_time, y=-.10, yend=-.10),
               inherit.aes = FALSE, color="gray57", size = 2) +
    ga + ap +  theme(legend.text = element_text(size = 18),
               legend.title=element_text(size=20, face = "bold"),
               legend.key.size = unit(1.5, 'lines')) +
    scale_color_manual(name="   drift rate",
                       values = c("green", "orange", "red")) +
    xlab("time") + ylab("") + 
    #ylim(0, -2) +
    geom_rect(data = rect, aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), 
              alpha = .1, inherit.aes = FALSE) +
  #  xlim(0, xlim_ub) + 
  theme(axis.text=element_blank()) + theme(axis.ticks=element_blank()) #+ 
  
drift_sample
```

```{r}
#ggsave("../paper/figs/drift_sample.png", drift_sample, width=11, height=6, dpi=700)
```

Simulating densities — commented bc it takes a long time to run  

```{r}
# a <- 1.93
# t <- .9
# b <- .5
# 
# #high_drift_samples <- 
# diff_drift_samples <- 
#   rbind(
#     data.frame(rwiener(5e4, alpha=a, tau=t, beta=b, delta =.7), type="high"),
#     data.frame(rwiener(5e4, alpha=a, tau=t, beta=b, delta =.04), type="medium"),
#     data.frame(rwiener(5e4, alpha=a, tau=t, beta=b, delta = -.7), type="low"))
# 
# diff_drift_samples[diff_drift_samples$resp=="lower", "q"] <- -diff_drift_samples[diff_drift_samples$resp=="lower", "q"] 
# 
# #diff_drift_samples$resp=="lower"
# test_diff_plot <- ggplot(diff_drift_samples, aes(x=q, color=type)) +
#   geom_density(size=3.5) + ga + ap + tol +
#   scale_color_manual(
#                        values = c("green", "orange", "red")) +
#   theme(axis.text = element_blank(), axis.ticks = element_blank()) + xlab("") + ylab("")
# test_diff_plot
```


```{r}
#ggsave("../paper/figs/diff_drift_dists_resize.png", test_diff_plot, width=35, height=3)
```

# Modeling    


```{r}
GetMapEsts <- function(s_df, format="long") {
  ### Returns dataframe of Maximum a Posteriori estimates for the variables in a dataframe.
  # s_df must be a subject-level dataframe (ie. excluding group-level estimates) ###
  
  # Handle entry of dfs other than subject-only traces 
  if (!all(grepl("subj.", names(s_df)))) stop("Must enter a trace data-frame with ONLY
                                              subject-level vars")
  
  # Posterior mean 
  col_means <- colMeans(s_df)

  # Map est 
  if (format == "long") {
    mdf <- foreach(i = 1:ncol(s_df)) %do% {
      this_par <- s_df[, i]
      
      mdf_row <- data.table(
        "ID"=unlist(map(strsplit(names(s_df)[i], "subj."), 2)),
        "var"=unlist(map(strsplit(names(s_df)[i], "subj."), 1)),
        "map_est"=as.numeric(bayestestR::map_estimate(this_par))
      )
    mdf_row 
    } %>% bind_rows()  
    
    z_IDs <- grep("trans", mdf$ID)
    if (length(z_IDs)) {
      mdf[grepl("trans", mdf$ID), "ID"] <- unique(mdf$ID[!grepl("trans", mdf$ID)])
    }
  }
  
  if (format == "short") {
    # Put in short format with each variable as a column name
    # First extract the unique vars that will serve as column names   
    unique_vars <- unique(unlist(map(strsplit(names(col_means), "subj."), 1)))
    unique_IDs <- unique(unlist(map(strsplit(names(col_means), "subj."), 2)))
    
    # Remove putative IDs with "trans" appended in case of z
    fake_IDs <- grep("trans", unique_IDs)
    if (length(fake_IDs)) unique_IDs <- unique_IDs[-c(fake_IDs)]
    
    map_est <- unlist(foreach(i = 1:ncol(s_df)) %do% {
      as.numeric(bayestestR::map_estimate(s_df[, i]))
    }) 
    
    mdf <- 
      setNames(data.frame(
        # Assign the ID as first column.. 
        unique_IDs,
        # ..and split into columns for each new variable
        matrix(map_est, ncol=length(unique_vars))              
      ), 
      # Assign column names via setNames
      c("ID", unique_vars))
  }
  mdf$ID <- as.numeric(as.character(mdf$ID))
  
  z_IDs <- grep("trans", mdf$ID)
  if (length(z_IDs)) {
    mdf[grepl("trans", mdf$ID), "ID"] <- unique(mdf$ID[!grepl("trans", mdf$ID)])
  }
  
  # Spot checks that estimates are v similar  
  #mdf[550:560]
  # col_means[550:560]
  # mdf$map_est[180:190]
  # col_means[180:190]
  # mdf$map_est[1:10]
  # col_means[1:10]

mdf
}
```


### Gelman Rubin of winning model  


```{r}
ReadTrace <- function(unique_str) read.csv(unique_str)

ndgr1 <- ReadTrace("../../model_res/final_traces_and_dics/traces/GR_run_also8k_ddm_add_trialwise_NO-SZ_s_vt_poutlier052177.csv")  
ndgr2 <- ReadTrace("../../model_res/final_traces_and_dics/traces/GR_run_also8k_ddm_add_trialwise_NO-SZ_s_vt_poutlier052923.csv")
ndgr3 <- ReadTrace("../../model_res/final_traces_and_dics/traces/GR_run_also8k_ddm_add_trialwise_NO-SZ_s_vt_poutlier056294.csv")
ndgr4 <- ReadTrace("../../model_res/final_traces_and_dics/traces/GR_run_also8k_ddm_add_trialwise_NO-SZ_s_vt_poutlier058876.csv")
```



```{r}
d1c <- rbind(ndgr1, ndgr2, ndgr3, ndgr4)
```

Fine  

```{r}
rhats <- unlist(foreach (i = 2:ncol(ndgr1)) %do% rstan::Rhat(cbind(
                                                                 unlist(ndgr1[i]), 
                                                                 unlist(ndgr2[i]), 
                                                                 unlist(ndgr3[i]), 
                                                                 unlist(ndgr4[i]))))

hist(rhats)
rhats[which(rhats > 1.1)]

max(rhats)
```

### Parameter recovery 


```{r}
recovered <- 
  read.csv("../../model_res/par_recov/HDDM_par_recov_traces_v-val_ddm_add_trialwise_NO-SZ_s_vt_with-z_1239.csv")
simmed <- read.csv("../../model_res/par_recov/HDDM_sims_for_pr_add_trialwise_NO-SZ_s_vt_poutlier059513.csv")
```

The across-trial variability parameters were run at the group level, so for these we'll look at the range of means used to generate alongside the recovered posterior  

```{r}
generative_sv <- data.frame(simmed %>% group_by(subj_idx) %>% summarize(msv=mean(sim_sv)))
sv_plot <- ggplot(recovered, aes(x=sv)) +
  geom_point(data=generative_sv, aes(x=msv, y=.2), size=3, pch=21, fill="gray57", color="black", alpha=.3) + 
  geom_point(data=generative_sv, aes(x=mean(msv), y=.2), size=5, pch=21, fill="gray57", color="black", alpha=.7) + 
  geom_density(color="gray57", fill="white", size=3, alpha=0) +
  ga +  lp + 
  xlab("") + ylab("") + 
  labs(title="Across-trial drift rate") +
  #+ #ylab("Higher values = more pos. > neg.") + 
  theme(axis.text.x = element_text(size=25), axis.ticks=element_blank(), axis.text.y=element_blank()) +
  theme(plot.title = element_text(margin=margin(b=-30), 
                    size=12, hjust=.05, vjust=0.1)) 

sv_plot
```


```{r}
# Add st  
generative_st <- data.frame(simmed %>% group_by(subj_idx) %>% summarize(mst=mean(sim_st)))

st_plot <- ggplot(recovered, aes(x=st)) +
  geom_point(data=generative_st, aes(x=mst, y=.2), size=3, pch=21, fill="gray57", color="black", alpha=.3) + 
  geom_point(data=generative_st, aes(x=mean(mst), y=.2), size=5, pch=21, fill="gray57", color="black", alpha=.7) + 
  geom_density(color="gray57", fill="white", size=3, alpha=0) +
  ga +  lp + 
  xlab("") + ylab("") + 
  labs(title="Across-trial non-decision time") +
  #+ #ylab("Higher values = more pos. > neg.") + 
  theme(axis.text.x = element_text(size=25), axis.ticks=element_blank(), axis.text.y=element_blank()) +
  theme(plot.title = element_text(margin=margin(b=-30), 
                    size=12, hjust=.05, vjust=0.1)) 

st_plot
```

For the rest of the parameters we had subject-level estimates, and we examine the MAP estimates of the recovered traces against the generative subject-level parameters   


```{r}
PlotParRecov <- function(sim_df, opt_df, sim_name, rec_name, parameter_label=NULL) {
  
  sim_df$simulated <- unlist(gen_vals[sim_name])
  sim_df$recovered <- unlist(opt_df[rec_name])
  
  cor_res <- cor.test(sim_df$simulated, sim_df$recovered)
  lower_lim <- min(sim_df$simulated, sim_df$recovered)
  upper_lim <- max(sim_df$simulated, sim_df$recovered)
  
  pr_p <- 
    ggplot(sim_df, aes(x=simulated, y=recovered)) + 
      geom_line(aes(x=simulated, y=simulated), size=2) +
      geom_point(size=7, alpha=.7, pch=21, fill="gray57") + 
      ga + ap + 
    labs(title=paste("r =", round(cor_res$estimate, 2)),
         subtitle=parameter_label) +
    theme(plot.subtitle = element_text(size = 25)) + 
    theme(axis.text = element_text(size = 15)) + 
     tp + 
    xlim(lower_lim, upper_lim) + ylim(lower_lim, upper_lim)
  
pr_p
}
```


```{r}
gen_vals <- simmed %>% group_by(subj_idx) %>% summarize_at(
  names(simmed)[grep("sim", names(simmed))], mean
)
rec_map <- GetMapEsts(GetSubjTraces(recovered), "short")
rec_map$conv_z <- InvLogit(rec_map$z_) # Transform recov z back on generative scale
```

```{r}
ests <- c()
ests[1] <- cor.test(gen_vals$sim_a, rec_map$a_)$est
ests[2] <- cor.test(gen_vals$sim_t, rec_map$t_)$est
ests[3] <- cor.test(gen_vals$sim_z, rec_map$conv_z)$est
ests[4] <- cor.test(gen_vals$sim_vi, rec_map$v_Intercept_)$est
ests[5] <- cor.test(gen_vals$sim_vv, rec_map$v_val_ctr_)$est
ests[6] <- cor.test(gen_vals$sim_vs, rec_map$v_sess_ctr_)$est
ests[7] <- cor.test(gen_vals$sim_vsv, rec_map$v_val_ctr.sess_ctr_)$est

ests
median(ests)
range(ests)
```

```{r}
pr_plots <- list()

if (all(rec_map$ID==gen_vals$subj_idx)) {
  pr_plots[[1]] <- PlotParRecov(gen_vals, rec_map, "sim_a", "a_", "Threshold (a)")
  pr_plots[[2]] <- PlotParRecov(gen_vals, rec_map, "sim_t", "t_", "Non-decision time (t)")
  pr_plots[[3]] <- PlotParRecov(gen_vals, rec_map, "sim_z", "conv_z", "Starting point bias (z)")
  pr_plots[[4]] <- PlotParRecov(gen_vals, rec_map, "sim_vi", "v_Intercept_", "Drift rate intercept")
  pr_plots[[5]] <- PlotParRecov(gen_vals, rec_map, "sim_vv", "v_val_ctr_", "Drift rate valence")
  pr_plots[[6]] <- PlotParRecov(gen_vals, rec_map, "sim_vs", "v_sess_ctr_", "Drift rate time")
  pr_plots[[7]] <- PlotParRecov(gen_vals, rec_map, "sim_vsv", "v_val_ctr.sess_ctr_", "Drift rate valence * time")
}
```

```{r, fig.width=12}
pr_grid <- (pr_plots[[1]] + pr_plots[[2]]) /
  (pr_plots[[3]] + pr_plots[[4]]) /
  (pr_plots[[5]] + pr_plots[[6]]) 

odd_pr_out <- pr_plots[[7]] 

pr_grid
```

```{r}
#ggsave("../paper/figs/pr-grid_SUPP.png", pr_grid, width=15, height=11, dpi=800)
```

```{r}
odd_pr_out
```
```{r}
#ggsave("../paper/figs/odd-pr_SUPP.png", odd_pr_out, width=7.5, height=11/3, dpi=800)
```

```{r}
across_trial <- sv_plot + st_plot + plot_annotation(
  "Recovered group posterior and generative parameters (mean and individual)", 
  theme = theme(plot.title = element_text(size = 15)))
across_trial
```

```{r}
#ggsave("../paper/figs/across-trial_pr-SUPP.png", across_trial, width=8, height=4)
```

## Behavioral results  and reflection in modeling  

### RTs and Endorsements  

- Endorsements and RT shift by condition 

```{r}
bx_df$session <- factor(bx_df$session, levels=c("Pre", "Post")) 

pre_post_vends <- bx_df %>% 
  group_by(valence, session) %>% 
  summarize(mr=mean(response))

pre_post_vends_id <- bx_df %>% 
  group_by(valence, session, subj_idx) %>% 
  summarize(mr=mean(response))

pre_post_vends_id
pp_sds <- pre_post_vends_id %>% group_by(valence, session) %>% 
  summarize(ci95=qnorm(.975)*sd(mr)/sqrt(length(unique(bx_df$subj_idx))))

if (all(pp_sds[1:2] == pre_post_vends[1:2])) pre_post_vends$ci95 <- pp_sds$ci95

# Sanity check 
#length(unique(bx_df$subj_idx))
# pre_post_vends_id %>% filter(valence == "negative" & session == "Pre") %>% 
#   select(mr) %>% summarize(sd(mr))/sqrt(96)*qnorm(.975)
#qnorm(.975)

```


```{r}
#pre_post_vends_id <- bx_df %>% group_by(valence, session, subj_idx) %>% summarize(mr=mean(response))
ends_plot <- ggplot(pre_post_vends_id, aes(x=session, y=mr, group=session, fill=valence)) + 
  geom_jitter(pch=21, color="black", width=.16, size=3, alpha=.2) + 
  geom_line(data=pre_post_vends, aes(y=mr, group=valence), color="gray57", pch=21, size=1.5) + 
  geom_errorbar(data=pre_post_vends, aes(x=session, 
                                ymin=mr-ci95, ymax=mr+ci95), color="black", inherit.aes=FALSE, size=1.5, width=.25) +
  geom_point(data=pre_post_vends, aes(y=mr, fill=valence), pch=21, size=6, alpha=.8) + 
  ylab("proportion endorsements \n (with 95% CI)") + xlab("") +
  scale_fill_manual(values=c("red", "blue")) +
  ga + ap + tol 

ends_plot
#ggsave("../paper/figs/ends_plot.png", ends_plot, width=11, height=7)
```

```{r}
pre_post_rts <- bx_df %>% group_by(valence, session) %>% summarize(mr=mean(rt))
                                                                  
pre_post_rts_id <- bx_df %>% group_by(valence, session, subj_idx) %>% summarize(mr=mean(rt))

pprt_sds <- pre_post_rts_id %>% group_by(valence, session) %>% 
  summarize(ci95=qnorm(.975)*sd(mr)/sqrt(length(unique(bx_df$subj_idx))))

if (all(pprt_sds[1:2] == pre_post_rts[1:2])) pre_post_rts$ci95 <- pprt_sds$ci95

rts_plot <- ggplot(pre_post_rts_id, aes(x=session, y=mr, group=session, fill=valence)) + 
    geom_jitter(pch=21, color="black", width=.16, size=3, alpha=.2) + 
  geom_line(data=pre_post_rts, aes(y=mr, group=valence), color="gray57", pch=21, size=1.5) + 
  geom_errorbar(data=pre_post_rts, aes(x=session, 
                                ymin=mr-ci95, ymax=mr+ci95), color="black", inherit.aes=FALSE, size=1.5, width=.25) +
  geom_point(data=pre_post_rts, aes(y=mr, fill=valence), pch=21, size=6, alpha=.8) +
  ylab("mean reaction time \n (with 95% CI)") + xlab("") +
  scale_fill_manual(values=c("red", "blue")) +
  ga + ap + tol 

rts_plot
#ggsave("../paper/figs/rt_plot.png", rts_plot, width=11, height=7)
```

Regression models  

```{r}
#scale(bx_df$valence_n)
summary (m1 <- glmer(response ~ val_ctr*sess_ctr + 
        (1|subj_idx), data=bx_df, family="binomial", control = glmerControl(optimizer = "bobyqa")))

car::vif(m1)
# Singular - unsuprisingly bc there's just 2 levels of each per subj
# summary (m2 <- glmer(response ~ val_ctr*sess_ctr + 
#         (val_ctr + sess_ctr|subj_idx), data=bx_df, family="binomial", control = glmerControl(optimizer = "bobyqa")))
```

Using this one bc includes sensible and important RE and not singular  
```{r}
summary (m3 <- glmer(response ~ val_ctr*sess_ctr + 
        (val_ctr|subj_idx), data=bx_df, family="binomial", control = glmerControl(optimizer = "bobyqa")))

car::vif(m3)
```

```{r}
summary (rm1 <- lmer(rt ~ val_ctr*sess_ctr + 
        (1|subj_idx), data=bx_df))
car::vif(rm1)
```


Used for same reason  

```{r}
summary (rm2 <- lmer(rt ~ val_ctr*sess_ctr + 
        (val_ctr|subj_idx), data=bx_df))
car::vif(rm1)
```


### DIC  

```{r}
model_names <- c("baseline", "+ valence", "+ valence * time", "+ bias", "+ sv and st")

dic_values <-
  c(
    read.csv("./../../model_res/final_traces_and_dics/dic/s_b_389_dic.csv", header=FALSE)$V2,
    read.csv("./../../model_res/final_traces_and_dics/dic/s_b1_3987_dic.csv", header=FALSE)$V2,
    read.csv("./../../model_res/final_traces_and_dics/dic/s_vt_dic.csv", header=FALSE)$V2,
    read.csv("./../../model_res/final_traces_and_dics/dic/GR_8k_s_vt_poutlier058886dic.csv", header=FALSE)$V2,
    read.csv("./../../model_res/final_traces_and_dics/dic/GR_run_also8k_ddm_add_trialwise_NO-SZ_s_vt_poutlier056294dic.csv", header=FALSE)$V2)

dic_df <- data.frame(model_names, dic_values)
dic_df$delta_dic <- c(0, dic_values[2:5]-dic_values[1])
#dic_df$delta_dic <- as.numeric(c(0, unlist(dic_df[2:nrow(dic_df), 1]) - unlist(dic_df[1, 1])))
dic_df$model_names <- factor(dic_df$model_names, levels=c("baseline", "+ valence", "+ valence * time", "+ bias", "+ sv and st"))

dic_p <- ggplot(dic_df[2:5, ], aes(x=model_names, y=delta_dic)) +
  geom_bar(stat="identity", color="black", fill="white", size=2) +
  ga + ap  + tol +
  ylab(TeX("$\\Delta$ DIC")) +
  xlab("") +
  labs(title = "Model comparison",
              subtitle = "Relative to baseline (no regressors) model") +
  theme(plot.subtitle = element_text(size = 15)) +
  #ggtitle("Model comparison relative to baseline") +
  tp +
  theme(axis.text.x = element_text(angle=30, hjust=1, size=20))#+ xlim(500, -1200)
dic_p
```

```{r}
#ggsave("../paper/figs/dic.png", dic_p, width=8, height=6, dpi=600)
```



### Posteriors  


```{r}
val_posteriors <- ggplot(d1c, aes(x=v_val_ctr)) +
  geom_vline(xintercept=0, size=2) +
  geom_density(color="chocolate", fill="white", alpha=1, size=3) +
  geom_density(aes(x=v_val_ctr.sess_ctr), color="tan1", fill="white", alpha=1, size=3) +
  xlim(-.1, 2) + #geom_vline(xintercept = 0, color="gray57", size=2) 
  ga +  lp + 
  xlab("") + ylab("") + #ylab("Higher values = more pos. > neg.") + 
  labs(title="Drift valence (dark) and valence*time (light) \n  regressor posteriors") +
  theme(axis.text.x = element_text(size=35), axis.ticks=element_blank(), axis.text.y=element_blank()) +
  theme(plot.title = element_text(margin=margin(b=-30), 
                    size=30, hjust=.18, vjust=.6)) 

val_posteriors
```

```{r}
#ggsave("../paper/figs/val_posteriors.png", val_posteriors, width=12, height=7)
```

```{r}
bayestestR::map_estimate(d1c$v_val_ctr)
which(d1c$v_val_ctr < 0)
#hist(d1c$v_val_ctr)

bayestestR::map_estimate(d1c$v_val_ctr.sess_ctr)
which(d1c$v_val_ctr.sess_ctr < 0)
#hist(d1c$v_val_ctr.sess_ctr)
```



```{r}
nends_b <- bx_df %>% filter(session=="Pre", valence=="negative") %>% 
  group_by(subj_idx) %>% summarize(m=mean(response), mrt=mean(rt))

pends_b <- bx_df %>% filter(session=="Pre", valence=="positive") %>% group_by(subj_idx) %>% 
  summarize(m=mean(response), mrt=mean(rt))

if (all(pends_b$subj_idx==nends_b$subj_idx)) {
  pends_b$pn_diff <- pends_b$m-nends_b$m
  pends_b$pn_rt_diff <- pends_b$mrt-nends_b$mrt
}
# pends_b$mrt
# nends_b$mrt
```


```{r}
rt <- bx_df %>% 
  group_by(subj_idx) %>% summarize(m=mean(rt), mrt=mean(rt))

rt_val <- bx_df %>% 
  group_by(subj_idx, valence) %>% summarize(mrt=mean(rt))

rt_val_short <- data.frame(
  "ID"=unique(rt_val$subj_idx),
  "overall_rt_diff"=rt_val[rt_val$valence == "positive", "mrt"]-rt_val[rt_val$valence == "negative", "mrt"])

nends_p <- bx_df %>% filter(session=="Post", valence=="negative") %>% 
  group_by(subj_idx) %>% summarize(m=mean(response), mrt=mean(rt))

pends_p <- bx_df %>% filter(session=="Post", valence=="positive") %>% group_by(subj_idx) %>% summarize(m=mean(response), mrt=mean(rt))

if (all(pends_p$subj_idx==nends_p$subj_idx)) {
  pends_p$pn_diff <- pends_p$m-nends_p$m
  pends_p$pn_rt_diff <- pends_p$mrt-nends_p$mrt
}
```



```{r}
d1m <- GetMapEsts(GetSubjTraces(d1c), "short")

rt <- bx_df %>% 
  group_by(subj_idx) %>% summarize(m=mean(rt), mrt=mean(rt))


if (all(d1m$ID==pends_p$subj_idx) & all(d1m$ID == pends_b$subj_idx)) {

  cat("\n DDM baseline ends"); print(cor.test(d1m$v_val_ctr_, pends_b$pn_diff))
  cat("\n DDM baseline rt"); print(cor.test(d1m$v_val_ctr_, pends_b$pn_rt_diff))

  cat("\n DDM change ends");print(cor.test(d1m$v_val_ctr.sess_ctr_, pends_p$pn_diff-pends_b$pn_diff))
  cat("\n DDM change rt"); print(cor.test(d1m$v_val_ctr.sess_ctr_, pends_p$pn_rt_diff-pends_b$pn_rt_diff))
} 
# This is the overall RT valence RT averaging over time point 
if (all(d1m$ID == rt_val_short$ID)) cat("\n DDM val rt"); print(cor.test(d1m$v_val_ctr_, rt_val_short$mrt))

if (all(d1m$ID == rt$subj_idx)) cat("\n DDM threshold - rt"); print(cor.test(d1m$a_, rt$m))
```



## Model validation w/ PPCs  

```{r}
s_bdf <- bx_df
```

```{r}
m1_ddm_ppcs <- read.csv("../../model_res/ppcs/HDDM_ppcs_v-val_ddm_add_trialwise_NO-SZ_s_vt_with-z_9533.csv")
```

Recode response 

```{r}
m1_ddm_ppcs[m1_ddm_ppcs$sess_ctr < 0, "session"] <- "Pre"
m1_ddm_ppcs[m1_ddm_ppcs$sess_ctr > 0, "session"] <- "Post"
m1_ddm_ppcs$session <- factor(m1_ddm_ppcs$session, levels=c("Pre", "Post"))

m1_ddm_ppcs[m1_ddm_ppcs$val_ctr > 0, "valence"] <- "positive"
m1_ddm_ppcs[m1_ddm_ppcs$val_ctr < 0, "valence"] <- "negative"
m1_ddm_ppcs$valence <- factor(m1_ddm_ppcs$valence, levels=c("negative", "positive"))
#qp_no_t$response <- factor(qp_no_t$response, levels=c("no", "yes"))
```

```{r}
hist(m1_ddm_ppcs$rt, breaks = 100, xlim = c(0, 4))
```

Quantiles look pretty good 
```{r}
quantile(s_bdf$rt, probs = seq(0.05, .95, .1))
quantile(m1_ddm_ppcs$rt, probs = seq(0.05, .95, .1))
```


Code flip rt for PPC plots  
```{r}
s_bdf$flip_rt <- s_bdf$rt
s_bdf[s_bdf$response==0, "flip_rt"] <- -s_bdf[s_bdf$response==0, "flip_rt"] 

m1_ddm_ppcs$flip_rt <- m1_ddm_ppcs$rt
m1_ddm_ppcs[m1_ddm_ppcs$response==0, "flip_rt"] <- -m1_ddm_ppcs[m1_ddm_ppcs$response==0, "flip_rt"] 

m1_ddm_ppcs$flip_rt <- m1_ddm_ppcs$rt
m1_ddm_ppcs[m1_ddm_ppcs$response==0, "flip_rt"] <- -m1_ddm_ppcs[m1_ddm_ppcs$response==0, "flip_rt"] 
```


Cap the PPCs at 3 because that's where the task cut off 

```{r}
m1_ddm_ppcs <- m1_ddm_ppcs %>% filter(rt < 3)
```

Create alt negative coded rt with negative side flipped so lines up with QP plots   


```{r}
m1_ddm_ppcs$alt_rt <- m1_ddm_ppcs$flip_rt

s_bdf$alt_rt <- s_bdf$flip_rt
#m1_ddm_ppcs

m1_ddm_ppcs[m1_ddm_ppcs$valence=="negative", "alt_rt"] <- -m1_ddm_ppcs[m1_ddm_ppcs$valence=="negative", "alt_rt"]

s_bdf[s_bdf$valence=="negative", "alt_rt"] <- -s_bdf[s_bdf$valence=="negative", "alt_rt"]
```


```{r}
ddm_alt <- ggplot(s_bdf, aes(x=alt_rt)) +
  geom_density(alpha=.1, size=1.3, color="black", fill="black") +
  geom_density(data=m1_ddm_ppcs, alpha=0, color="darkorange", 
               size=1.3, fill="red", linetype="longdash") + 
  facet_wrap( ~ valence) +
  ga + ap + ft + tp + xlab("reaction time") + xlim(-3.1, 3.1) + 
  theme(axis.text=element_blank(), axis.ticks = element_blank()) + 
  ylab("") 
  
ddm_alt
```


```{r}
#ggsave("../paper/figs/big_ppc_alt.png", ddm_alt, width=9.25, height=3)
```


X axis - response frequency 

```{r}
ddm_neg <- 
  table(m1_ddm_ppcs[m1_ddm_ppcs$valence=="negative", "response"])[2]/
    sum(table(m1_ddm_ppcs[m1_ddm_ppcs$valence=="negative", "response"]))

ddm_pos <- 
  table(m1_ddm_ppcs[m1_ddm_ppcs$valence=="positive", "response"])[2]/
    sum(table(m1_ddm_ppcs[m1_ddm_ppcs$valence=="positive", "response"]))

emp_neg <- 
  table(s_bdf[s_bdf$valence=="negative", "response"])[2]/
    sum(table(s_bdf[s_bdf$valence=="negative", "response"]))

emp_pos <- 
  table(s_bdf[s_bdf$valence=="positive", "response"])[2]/
    sum(table(s_bdf[s_bdf$valence=="positive", "response"]))
```

Y axis  - reaction time quantiles 

```{r}
qs <- seq(0.1, .9, .2)
```

```{r}
CalculateRTQuantile <- function(val_resp_df, qs) {
  ### Get subject average quantiles at a given valence-response level ###
  
  all_subj_quantiles <- 
    lapply(split(val_resp_df, val_resp_df$subj_idx), function(x) {
      
      out <- 
        data.table(unique(x$subj_idx), 
                   unique(x$valence), 
                   unique(x$response), 
                   quantile(x$rt, probs = qs),
                   qs) %>% 
        setNames(c("ID", "valence", "response", "quantiles", "qs"))
    out  
    }) %>% bind_rows()
  
all_subj_quantiles
}
```


```{r}
ddm <- 
    data.table(
      rbind(
        CalculateRTQuantile(m1_ddm_ppcs %>% filter(valence=="negative", response==1), qs),
        CalculateRTQuantile(m1_ddm_ppcs %>% filter(valence=="positive", response==1), qs),
        CalculateRTQuantile(m1_ddm_ppcs %>% filter(valence=="negative", response==0), qs),
        CalculateRTQuantile(m1_ddm_ppcs %>% filter(valence=="positive", response==0), qs)),
        "model"="ddm"
  )


emp <- data.table(
    rbind(
      CalculateRTQuantile(s_bdf %>% filter(valence=="negative", response==1), qs),
      CalculateRTQuantile(s_bdf %>% filter(valence=="positive", response==1), qs),
      CalculateRTQuantile(s_bdf %>% filter(valence=="negative", response==0), qs),
      CalculateRTQuantile(s_bdf %>% filter(valence=="positive", response==0), qs)),
      "model"="empirical")  


all_subj_qs <- rbind(ddm, emp)
```


Put it all together  

```{r}
q_rt_sum <- all_subj_qs %>% group_by(model, valence, response, qs) %>% summarize(q=mean(quantiles))
#q_rt_sum %>% filter(response==1 && valence=="negative", model=="ddm")
#ddm_neg
qp_df <- 
  rbind(
    ## DDM
    # For each model-valence, use the RT quantiles at response 1 and 0, whose response probs are p(endorse) and 1-p(endorse)
    q_rt_sum %>% filter(response==1 && valence=="negative", model=="ddm") %>% mutate(rp=ddm_neg),
    q_rt_sum %>% filter(response==0 && valence=="negative", model=="ddm") %>% mutate(rp=1-ddm_neg),
    
    q_rt_sum %>% filter(response==1 && valence=="positive", model=="ddm") %>% mutate(rp=ddm_pos),
    q_rt_sum %>% filter(response==0 && valence=="positive", model=="ddm") %>% mutate(rp=1-ddm_pos), 
    
    ## Empirical
    q_rt_sum %>% filter(response==1 && valence=="negative", model=="empirical") %>% mutate(rp=emp_neg),
    q_rt_sum %>% filter(response==0 && valence=="negative", model=="empirical") %>% mutate(rp=1-emp_neg),
    
    q_rt_sum %>% filter(response==1 && valence=="positive", model=="empirical") %>% mutate(rp=emp_pos),
    q_rt_sum %>% filter(response==0 && valence=="positive", model=="empirical") %>% mutate(rp=1-emp_pos)
  )
```

```{r}
qp_plot <- ggplot(qp_df, aes(x=rp, y=q, fill=model)) + geom_point(pch=21, size=6, alpha=.7) + 
  facet_wrap( ~ valence) + ga + ap + lp + ft + 
  ylab("reaction time") + xlab("response frequency") +
  scale_fill_manual(values=c("black", "darkorange"), labels=c("Empirical", "DDM")) +
  theme(axis.text.x = element_text(angle=30, hjust=1, size=20)) + 
  theme(legend.position = c(.25, .65)) #+ xlim(500, -1200)
  
qp_plot
```


```{r}
#ggsave("../paper/figs/qp_plot.png", qp_plot, width=10, height=4.25)
```


```{r}
# probably don't need this plot 
# ppc_lf <- rbind(
#   data.table(m1_ddm_ppcs %>% select("subj_idx", "response", "valence", "flip_rt"), "ddm"),
#   data.table(lm1_ppc %>% select("subj_idx", "response"="responses", "valence", "flip_rt"), "levy"),
#   data.table(s_bdf %>% select("subj_idx", "response", "valence", "flip_rt"), "empirical")
# ) %>% setNames(c("ID", "response", "valence", "flip_rt", "model"))
# 
# table(ppc_lf$response)
# 
# # Summarize neg yes
# ny <- ppc_lf %>% filter( valence=="negative") %>%  group_by(ID, model) %>% summarize(mr=mean(response))
# 
# 
# sd(as.numeric(unlist(ny[ny$model=="ddm", "mr"])))/sqrt(length(unique(bx_df$subj_idx)))
# nys <- ny %>% group_by(model) %>% summarize(m=mean(mr))
# 
# nys$se <- 
#   c(
#     sd(as.numeric(unlist(ny[ny$model=="ddm", "mr"])))/sqrt(length(unique(bx_df$subj_idx))),
#     sd(as.numeric(unlist(ny[ny$model=="levy", "mr"])))/sqrt(length(unique(bx_df$subj_idx))),
#     sd(as.numeric(unlist(ny[ny$model=="empirical", "mr"])))/sqrt(length(unique(bx_df$subj_idx)))
#   )
#   
#   # c(
#   #   sd(unlist(ppc_lf[ppc_lf$model=="ddm", "response"]))/sqrt(length(unique(bx_df$subj_idx))), # ** make sure right
#   #   sd(unlist(ppc_lf[ppc_lf$model=="levy", "response"]))/sqrt(length(unique(bx_df$subj_idx))),
#   #   sd(unlist(ppc_lf[ppc_lf$model=="empirical", "response"]))/sqrt(length(unique(bx_df$subj_idx))))
# 
# 
# pd <- .25
# ny$model <- factor(ny$model, levels=c("ddm", "empirical", "levy"))
# neg <- ggplot(ny, aes(x=model, y=mr, group=ID, fill=model)) + 
#   #geom_line(position=position_dodge(width=.1), alpha=.16) + 
#   geom_point(width=.1, position=position_dodge(width=.18), alpha=1, size = 2, pch=21) + 
#   scale_fill_manual(values=c("orange", "gray57", "purple")) + 
#   # geom_errorbar(data=nys, aes(x=model,  ymax=m+se, ymin=m-se), inherit.aes = FALSE, size=2.2, width=.15, color="blue") +
#   geom_point(data=nys, aes(x=model, y=m), inherit.aes = FALSE, size=8, pch=21, fill="white") + 
#   
#   geom_hline(yintercept = c(unlist(nys[nys$model=="empirical", "m"])), size=1.2) +  
#   ylim(0, 1) + ga + ap + tol + xlab("") + ylab("response \n frequency") + 
#   theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.title = element_text(size=55))
# 
# neg
# 
# nys

# Summarize neg yes
# ny <- ppc_lf %>% filter( valence=="positive") %>%  group_by(ID, model) %>% summarize(mr=mean(response))
# 
# nys <- ny %>% group_by(model) %>% summarize(m=mean(mr))
# 
# nys$se <- 
#   c(
#     sd(as.numeric(unlist(ny[ny$model=="ddm", "mr"])))/sqrt(length(unique(bx_df$subj_idx))),
#     sd(as.numeric(unlist(ny[ny$model=="levy", "mr"])))/sqrt(length(unique(bx_df$subj_idx))),
#     sd(as.numeric(unlist(ny[ny$model=="empirical", "mr"])))/sqrt(length(unique(bx_df$subj_idx)))
#   )
# # 
# #   c(
# #     sd(unlist(ppc_lf[ppc_lf$model=="ddm", "response"]))/sqrt(length(unique(bx_df$subj_idx))), # ** make sure right
# #     sd(unlist(ppc_lf[ppc_lf$model=="levy", "response"]))/sqrt(length(unique(bx_df$subj_idx))),
# #     sd(unlist(ppc_lf[ppc_lf$model=="empirical", "response"]))/sqrt(length(unique(bx_df$subj_idx))))
# 
# 
# pd <- .25
# ny$model <- factor(ny$model, levels=c("ddm", "empirical", "levy"))
# pos <- ggplot(ny, aes(x=model, y=mr, group=ID, fill=model)) + 
#    geom_point(width=.1, position=position_dodge(width=.18), alpha=1, size = 2, pch=21) + 
#   scale_fill_manual(values=c("orange", "gray57", "purple")) + 
#   # geom_errorbar(data=nys, aes(x=model,  ymax=m+se, ymin=m-se), inherit.aes = FALSE, size=2.2, width=.15, color="blue") +
#   geom_point(data=nys, aes(x=model, y=m), inherit.aes = FALSE, size=8, pch=21, fill="white") + 
#   
#   geom_hline(yintercept = c(unlist(nys[nys$model=="empirical", "m"])), size=1.2) + 
#   ylim(0, 1) + ga + ap + tol + xlab("") + ylab("") + 
#   theme(axis.text=element_blank(), axis.ticks=element_blank(), axis.title = element_text(size=40)) #+
#   #scale_fill_manual(values=c("black", "orange", "purple"), labels=c("Empirical", "DDM", "Lévy")) 
# #+ xlim(500, -1200)
# 
# #pos
# both <- neg+pos
# both

#ggsave("../paper/figs/ppc_freqs.png", both, width=18, height=6)
```



## Regression models and symptom analyses   

### Prep questionnaire items  


```{r}
completer_ids <- unique(bx_df$subj_idx)
item_qairre_df <- read.csv("./../../data/raw_files/justDASSandFFMQ_with_imputes_replaced_with_999.csv")
#all(item_qairre_df$ID.ffmq==item_qairre_df$ID.DASS) # Pulled from different excel sheets so this double check IDs lined up exactly
```


```{r}
# No reverse scoring 
dass_dep_qs <- c(3, 5, 10, 13, 16, 17, 21, 24, 26, 31, 34, 37, 38, 42)

iq_c <- item_qairre_df %>% filter(ID.ffmq %in% completer_ids)
```


**DASS**  

```{r}
iq_c_d <- iq_c[grep("DASS", names(iq_c))]
```
`



Time 1 - just depression subscale 
```{r}
# Time 1 
dd1 <- iq_c_d[paste0("DASS1.", dass_dep_qs)]
```


```{r}
#cor(dd1, use=complete.cases)
```


```{r}
# 1.26% of baseline data missing
sum(length(which(dd1==999)), length(which(is.na(dd1))))/(dim(dd1)[1]*dim(dd1)[2])*100
dd1 <- data.frame(dd1)
dd1[dd1==999] <- NA
which(is.na(rowSums(dd1))) # 4 pts have NAs 
# 1 all data missing, the other just a couple for each 
dd1[which(is.na(rowSums(dd1))), ]
# Put in ID  
dd1 <- data.frame("ID"=iq_c_d$ID.DASS, dd1)

# For robustness check, casewise delete pts with any data missing 
dd1_casewise_delete <- dd1[-which(is.na(rowSums(dd1))), ]
dd1_casewise_delete$dass_sum_score <- rowSums(dd1_casewise_delete[2:15])

#dd1[which((is.na(rowSums(dd1[2:15])))), ]
id_to_del <- 50 # This is the pt with all missing—since have no data, casewise delete this person
dd1 %>% filter(ID == id_to_del)
dd1 <- dd1[!dd1$ID==50, ]

# .21% of items missing after excluding all missing
length(which(is.na(dd1)))/(dim(dd1)[1]*dim(dd1)[2])*100

# Replace any NAs with the mean of the remainding items 
for (r in 1:nrow(dd1)) {
  if (any(is.na(dd1[r, 2:15]))) {
    this_row <- as.numeric(unlist(dd1[r, 2:15])) # Exclude the ID 
    # Mean to replace NAs with 
    mean_non_na <- mean(this_row[which(!is.na(this_row))])
    this_row[is.na(as.numeric(unlist(this_row)))] <- mean_non_na
    # Put in df
    dd1[r, 2:15] <- this_row 
  }    
}
dd1$dass_sum_score <- rowSums(dd1[2:15])
#gdf <- haven::read_sav("./../data/raw_files/Britton_ConcurrentStudyA_7.11.16.sav")
#gdf$DASS_Depression_SUM_Q1==rowSums(dd1)
```


Time 2

```{r}
# Time 2 
# 2.31 % of final data missing 
dd2 <- iq_c_d[paste0("DASS2.", dass_dep_qs)]
sum(length(which(dd2==999)), length(which(is.na(dd2))))/ (dim(dd2)[1]*dim(dd2)[2])*100

dd2[dd2==999] <- NA
which(is.na(rowSums(dd2))) # 3pts have NAs 
# 1 all data missing, the other just a couple for each 
dd2[which(is.na(rowSums(dd2))), ]
# Put in ID  
dd2 <- data.frame("ID"=iq_c_d$ID.DASS, dd2)

# For robustness check, casewise delete pts with any data missing 
dd2_casewise_delete <- dd2[-which(is.na(rowSums(dd2))), ]
dd2_casewise_delete$dass_sum_score <- rowSums(dd2_casewise_delete[2:15])


# Delete pts with all data missing 
ids_to_del <- c(56, 58) 
dd2 <- dd2 %>% filter(!ID %in% ids_to_del)

# .21% of items missing 
length(which(is.na(dd2)))/(dim(dd2)[1]*dim(dd2)[2])*100

# Replace the one person missing just a couple items with the mean
mean_to_repl <- mean(unlist(dd2[dd2$ID==59, 2:15][which(!is.na(dd2[dd2$ID==59, 2:15]))]))
dd2[dd2$ID==59, which(is.na(dd2[dd2$ID==59, ]))] <- mean_to_repl
  
dd2$dass_sum_score <- rowSums(dd2[2:15])
dd2$dass_sum_score

#any(is.na(dd2))
```


### Symptoms  

```{r}
dass_time_1 <- dd1
dass_time_2 <- dd2
```

Make sure the DASS are numerically ordered, then give them names  

```{r}
order <- as.numeric(unlist(
  lapply(names(dass_time_1)[grep("DASS", names(dass_time_1))], function(x) unlist(map(strsplit(x, "[.]"), 2)))
))

if (all(unlist(foreach(i = 1:(length(order)-1)) %do% { order[i] < order[i+1]}))) {
  extra_d1 <- dass_time_1[names(dass_time_1)[grep("DASS", names(dass_time_1))]] %>% 
    setNames(c(
      # 3, 5, 10, 13
      "no-positive-feelings", "couldnt-get-going", "easily-upset", "sad-and-depressed",
        # 16, 17, 21
        "lost-interest", "not-worth-much-as-person", "life-not-worthwhile", 
        # 24, 26, 31, 34, 37, 
        "no-enjoyment", "down-hearted-and-blue", "no-enthusiasm", "worthless", "no-hope", 
        #38, 42
        "life-meaningless", "no-initiative"
        ))
    
  dass_time_1 <- data.frame(extra_d1, dass_time_1)
}

# Spot checks 
#dass_time_1$no.initiative==dd1$DASS1.42
# dass_time_1$no.enjoyment==dd1$DASS1.24
# dass_time_1$life.meaningless==dd1$DASS1.38
# dass_time_1$sad.and.depressed==dd1$DASS1.13
```


```{r}
order2 <- as.numeric(unlist(
  lapply(names(dass_time_2)[grep("DASS", names(dass_time_2))], function(x) unlist(map(strsplit(x, "[.]"), 2)))
))
if (all(unlist(foreach(i = 1:(length(order2)-1)) %do% { order[i] < order2[i+1]}))) {
  extra_d2 <- dass_time_2[names(dass_time_2)[grep("DASS", names(dass_time_2))]] %>% 
    setNames(c(
      # 3, 5, 10, 13
      "no-positive-feelings", "couldnt-get-going", "easily-upset", "sad-and-depressed",
        # 16, 17, 21
        "lost-interest", "not-worth-much-as-person", "life-not-worthwhile", 
        # 24, 26, 31, 34, 37, 
        "no-enjoyment", "down-hearted-and-blue", "no-enthusiasm", "worthless", "no-hope", 
        #38, 42
        "life-meaningless", "no-initiative"
        ))
    
  dass_time_2 <- data.frame(extra_d2, dass_time_2)
}
# Spot checks 
#dass_time_2$no.initiative==dd2$DASS2.42
#dass_time_2$down.hearted.and.blue==dd2$DASS2.26
#dass_time_2$no.positive.feelings==dd2$DASS2.3
#dass_time_2$sad.and.depressed==dd2$DASS2.13
```

Create a long-form version 

```{r}
d1_short <- dass_time_1 %>% filter(ID %in% dass_time_2$ID)

d1_lf <- d1_short[1:(length(dass_dep_qs)+1)] %>%
pivot_longer(cols = no.positive.feelings:no.initiative,
 names_to = "item",
 values_to = "scores"
)
```


```{r}
d1_lf
```


```{r}
d2_short <- dass_time_2 %>% filter(ID %in% dass_time_1$ID)
if (all(d1_short$ID==d2_short$ID)) {
  change_extra <- d2_short[1:length(dass_dep_qs)] - d1_short[1:length(dass_dep_qs)]
  change_df <- data.frame(
    change_extra, change_extra %>% setNames(paste0("change_", names(change_extra))))
}


# Changes the order so not working yet  
labs <- unlist(lapply(strsplit(unique(d1_lf$item), "[.]"), function(x) {
  out <- paste0(x, collapse=" ")
out
}))

```


Drop the few imputed ones for vis  
```{r}
d1_lf <- d1_lf[-c(which(d1_lf$scores %% 1 != 0)), ]
```


Baseline  
```{r}
d1_summs <- d1_lf %>% group_by(item) %>% summarize(m=mean(scores)) 
d1_summs_ID <- d1_lf %>% group_by(item, ID) %>% summarize(m=mean(scores)) 

d1_sds <- d1_lf %>% group_by(item) %>% 
  summarize(ci95=qnorm(.975)*sd(scores)/sqrt(length(unique(bx_df$subj_idx))))
d1_sds
if (all(d1_summs$item==d1_sds$item)) d1_summs$ci95 <- d1_sds$ci95


d1_summs_arr <- 
  d1_summs %>% arrange(m)

#d1_summs_arr$item_f <- factor(ch_summs_arr$item, levels=rev(ch_summs_arr$item))

d1_lf$item_f <- factor(d1_lf$item, levels=c(d1_summs_arr$item))

base_labs <- as.character(levels(d1_lf$item_f))

#unlist(map(strsplit(levels(d1_lf$item_f), "change_"), 2))
b_labs <- unlist(lapply(strsplit(base_labs, "[.]"), function(x) {
  out <- paste0(x, collapse=" ")
out
}))
b_labs
```


```{r}
a_d <- ggplot(d1_lf, aes(x=scores, y=item_f)) + 
  geom_vline(xintercept=seq(0, 3, 1), size=1.5, color="gray57") +
  geom_jitter(alpha=.3, size=2.5, width=.1, height=0.28, pch=21, color="darkred", fill="white") +
  #geom_hline(yintercept=seq(1, 15, 1)) +
  geom_errorbar(data=d1_summs, aes(x=m, y=item,
                xmin=m-ci95, xmax=m+ci95), color="darkred", 
                inherit.aes=FALSE, size=2.2, width=.4) +
  geom_point(data=d1_summs, aes(x=m, y=item), pch=21, 
             fill="white", color="black", size=6, alpha=.9) + 
  ga + ap + theme(axis.text.y =element_text(size=11)) + 
  theme(axis.text.x =element_text(size=15)) +
  theme(axis.title.x =element_text(size=18)) + 
  ylab("") + 
  labs(title="Depression symptoms \nat baseline") + tp +
  xlab("ratings") +
  scale_y_discrete(labels=b_labs)
```

```{r}
#d1_summs # Spot check that life meaningless and life worthless are low means , easily upset in middle, couldnt get going at top 
```



```{r}
a_d
```

Change  
```{r}
change_for_p <- data.frame("ID"=d2_short$ID, change_df[grep("change", names(change_df))])
```

```{r}
ch_lf <- change_for_p %>%
 pivot_longer(cols = change_no.positive.feelings:change_no.initiative,
   names_to = "item",
   values_to = "scores"
 )

ch_lf <- ch_lf[-c(which(ch_lf$scores %% 1 != 0)), ]
#which(ch_lf$scores %% 1 !=0)
ch_summs <- ch_lf %>% group_by(item) %>% summarize(m=mean(scores)) 

ch_sds <- ch_lf %>% group_by(item) %>% 
  summarize(ci95=qnorm(.975)*sd(scores)/sqrt(length(unique(bx_df$subj_idx))))

if (all(ch_summs$item==ch_sds$item)) ch_summs$ci95 <- ch_sds$ci95
ch_summs_arr <- 
  ch_summs %>% arrange(m)

ch_summs_arr$item_f <- factor(ch_summs_arr$item, levels=rev(ch_summs_arr$item))
```


```{r}
ch_lf$item_f <- factor(ch_lf$item, levels=rev(c(ch_summs_arr$item)))
```


```{r}
relab <- unlist(map(strsplit(levels(ch_lf$item_f), "change_"), 2))
sx_change_labs <- unlist(lapply(strsplit(unique(relab), "[.]"), function(x) {
  out <- paste0(x, collapse=" ")
out
}))
sx_change_labs
```


```{r}
b_d <- ggplot(ch_lf, aes(x=scores, y=item_f)) + 
  geom_vline(xintercept=seq(-3, 3, 1), size=1.5, color="gray57") +
  geom_jitter(alpha=.3, size=2.5, width=.1, height=0.28, pch=21, color="red", fill="white") +
  #geom_hline(yintercept=seq(1, 15, 1)) +
  geom_errorbar(data=ch_summs_arr, aes(x=m, y=item,
                xmin=m-ci95, xmax=m+ci95), color="red", 
                inherit.aes=FALSE, size=2.2, width=.4) +
  geom_point(data=ch_summs_arr, aes(x=m, y=item), pch=21, 
             fill="white", color="black", size=6, alpha=.8) + 
  ga + ap + theme(axis.text.y =element_text(size=11)) + 
  theme(axis.title.x =element_text(size=18)) + 
  theme(axis.text.x =element_text(size=15)) +
  ylab("") + 
  labs(title="Change in depression symptoms \n(post - pre)") + tp +
  xlab(TeX("$\\Delta$ ratings")) +
  #scale_x_discrete(labels=bn) +
  scale_y_discrete(labels=sx_change_labs)
```


```{r}
# Spot check
# names(d2_short)==names(d1_short)
# data.frame(colMeans(d2_short-d1_short)[1:14])
```

```{r}
b_d
```

```{r}
ch_lf
```


```{r}
unique_items <- unique(d1_lf$item)
d1m_s <- d1m %>% filter(ID %in% d1_lf$ID)
all_res <- foreach (i = 1:length(unique_items)) %do% {
  this_item <- d1_lf %>% filter(item==unique_items[i])
  d1m_s_tmp <- d1m_s %>% filter(ID %in% unique(this_item$ID))
  #print(this_item)
  if (all(this_item$ID==d1m_s_tmp$ID)) {
    res <- cor.test(this_item$scores, d1m_s_tmp$v_val_ctr_)
    out <- data.table("item"=unique(this_item$item),
         "r"=as.numeric(res$estimate),
         "ci95_low"=as.numeric(res$conf.int[1]),
         "ci95_high"=as.numeric(res$conf.int[2]),
         "p"=as.numeric(res$p.value))
  } else {
    
  }
out
} %>% bind_rows()
all_res
```


```{r}
bonferroni <- .05/length(unique(d1_lf$item))
all_res$bonferroni <- (all_res$p < bonferroni)*1

all_res[which(all_res$p < .05), "signif"] <- "*"
all_res[which(all_res$p < .01), "signif"] <- "**"
all_res[which(all_res$p < .005), "signif"] <- "***"
all_res[which(all_res$p < .001), "signif"] <- "****"
all_res[which(all_res$p < .0005), "signif"] <- "*****"
all_res[which(is.na(all_res$signif)), "signif"] <- " "
```

```{r}
all_res_arr <- all_res %>% arrange(r)
all_res_arr$item <- factor(all_res_arr$item, levels=rev(c(all_res_arr$item)))
all_res_arr
```

```{r}
all_res_arr %>% select(item, bonferroni, signif)
```
```{r}
#ch_lf$item_f <- factor(ch_lf$item, levels=rev(c(ch_summs_arr$item)))
#relab <- unlist(map(strsplit(levels(ch_lf$item_f), "change_"), 2))
r_b_labs <- as.character(levels(all_res_arr$item))
r_b_labs
rb_labs <- unlist(lapply(strsplit(r_b_labs, "[.]"), function(x) {
  out <- paste0(x, collapse=" ")
out
}))
rb_labs
```

```{r}
a_r <- ggplot(all_res_arr, aes(x=r, y=item)) +
  geom_vline(xintercept = seq(-1, .2, .1), color="gray57", linetype="dotted") +
  geom_vline(xintercept = c(-1, 0), color="black", size=1.4) +
  
  geom_errorbar(data=all_res_arr, size=1.4, width=.19, color="gray42",
                aes(x=r, y=item, xmin=ci95_low, xmax=ci95_high)) + 
  #geom_hline(yintercept=seq(1, 15, 1)) +
  geom_point(size=6, fill="gray60", pch=21, alpha=.9) + 
  xlim(-1.15, .22) +
  geom_text(x=-1.1, y=14, size=6, label="X*****") +
  geom_text(x=-1.1, y=13, size=6, label="X*****") +
  geom_text(x=-1.1, y=12, size=6, label="X  ***") +
  geom_text(x=-1.1, y=11, size=6, label="X  ***") +
  geom_text(x=-1.1, y=10, size=6, label="X  ***") +
  geom_text(x=-1.1, y=9, size=6, label="X  ***") +
  geom_text(x=-1.1, y=8, size=6, label="      **") +
  geom_text(x=-1.1, y=7, size=6, label="      **") +
  geom_text(x=-1.1, y=6, size=6, label="       *") +
  geom_text(x=-1.1, y=5, size=6, label="       *") +
  geom_text(x=-1.1, y=4, size=6, label="       *") +
  geom_text(x=-1.1, y=3, size=6, label="       *") +
  
  ga + theme(axis.text.y =element_text(size=11), axis.text.x=element_text(size=15),
             axis.title = element_text(size=20)) + 
  ylab("") + 
  labs(title="Correlations of baseline depression symptoms with \n positive > negative drift rate estimates") + 
  theme(plot.title = element_text(size = 15, face='bold', hjust = .5)) +
  xlab("Pearson's r (with 95% CIs)") +
  scale_y_discrete(labels=rb_labs)
a_r
```



```{r}
unique_ch_items <- unique(ch_lf$item)

d1m_s_ch <- d1m %>% filter(ID %in% ch_lf$ID)
#d1m_s_ch
all_res_ch <- foreach (i = 1:length(unique_ch_items)) %do% {
  this_ch_item <- ch_lf %>% filter(item==unique_ch_items[i])
  d1m_s_ch_tmp <- d1m_s_ch %>% filter(ID %in% unique(this_ch_item$ID))
  #print(this_item)
  if (all(this_ch_item$ID==d1m_s_ch_tmp$ID)) {
    res <- cor.test(this_ch_item$scores, d1m_s_ch_tmp$v_val_ctr.sess_ctr_)
    out <- data.table("item"=unique(this_ch_item$item),
         "r"=as.numeric(res$estimate),
         "ci95_low"=as.numeric(res$conf.int[1]),
         "ci95_high"=as.numeric(res$conf.int[2]),
         "p"=as.numeric(res$p.value))
  } else {
    
  }
out
} %>% bind_rows()
```


```{r}
bonferroni_ch <- .05/length(unique(ch_lf$item))
all_res_ch$bonferroni_ch <- (all_res_ch$p < bonferroni_ch)*1

all_res_ch[which(all_res_ch$p < .05), "signif"] <- "*"
all_res_ch[which(all_res_ch$p < .01), "signif"] <- "**"
all_res_ch[which(all_res_ch$p < .005), "signif"] <- "***"
all_res_ch[which(all_res_ch$p < .001), "signif"] <- "****"
all_res_ch[which(is.na(all_res_ch$signif)), "signif"] <- " "
#all_res_ch
arc_arr <- all_res_ch %>% arrange(r)
arc_arr
arc_arr$item_f <- factor(arc_arr$item, levels=rev(as.character(arc_arr$item)))
#arc_arr$item_f <- factor(arc_arr$item, levels=as.character(arc_arr$item))
arc_arr$item_f

```


```{r}
r_ch_labs <- as.character(levels(arc_arr$item_f))

rch_labs <- unlist(lapply(strsplit(r_ch_labs, "[.]"), function(x) {
  
  out <- paste0(x, collapse=" ")
  out <- unlist(map(strsplit(out, "change_"), 2))
out
}))
rch_labs
```
```{r}
# all_res_ch # Spot checked 
```

```{r}
b_r <- ggplot(arc_arr, aes(x=r, y=item_f)) +
  geom_text(x=-1.1, y=14, size=6, label="X ***") +
  geom_text(x=-1.1, y=13, size=6, label="X ***") +
  geom_text(x=-1.1, y=12, size=6, label="   ***") +
  geom_text(x=-1.1, y=11, size=6, label="    **") +
  # geom_text(x=-.35, y=10, size=6, label="    **") +
  # geom_text(x=-.35, y=9, size=6, label="     *") +
  geom_vline(xintercept = seq(-1, .2, .1), color="gray57", linetype="dotted") +
  geom_vline(xintercept = c(-1, 0), color="black", size=1.4) +
  xlim(-1.15, .22) +
  geom_errorbar(data=arc_arr, size=1.4, width=.19, color="gray75",
                aes(x=r, y=item_f, xmin=ci95_low, xmax=ci95_high)) + 
  
  geom_point(size=6, fill="gray85", pch=21, alpha=.9) + 
  ga + 
  theme(axis.text.y =element_text(size=11), axis.text.x=element_text(size=15),
             axis.title = element_text(size=20)) + 
  ylab("") + 
  labs(title="Correlations of change in depression symptoms with \n positive > negative drift rate * time  estimates") + 
  theme(plot.title = element_text(size = 15, face='bold', hjust = .5)) +
  xlab("Pearson's r (with 95% CIs)") +
  scale_y_discrete(labels=rch_labs)
b_r
```



```{r, fig.height=14, width=10}
full_sxs_p <- a_d / b_d/
  a_r / b_r
#full_sxs_p
```


```{r}
#ggsave("../paper/figs/full_sxs.png", full_sxs_p, dpi=700, width=8, height=16)
```

Get Cronbach's alpha  

```{r}
psych::alpha(dd1[2:15])
```

```{r}
psych::alpha(dd2[2:15])
```


**FFMQ**  

```{r}
iq_c_m <- iq_c[grep("FFMQ", names(iq_c))]
# Reverse code items 
rev_items <- c(3, 5, 8, 10, 12, 13, 14, 16, 17, 18, 22, 23, 25, 28, 30, 34, 35, 38, 39)
# Check 
#sort(c(3, 10, 14, 17, 25, 30, 35, 39, 5, 8, 13, 18, 23, 28, 34, 38, 12, 16, 22))==rev_items
aware <- c(5, 8, 13, 18, 23, 28, 34, 38)
non_judge <- c(3, 10, 14, 17, 25, 30, 35, 39)
non_react <- c(4, 9, 19, 21, 24, 29, 33)
observe <- c(1, 6, 11, 15, 20, 26, 31, 36)
```

Time 1 

```{r}
m1 <- iq_c_m[grep("FFMQ1.", names(iq_c_m))]
# 1.31% of baseline data missing
sum(length(which(m1==999)), length(which(is.na(m1))))/(dim(m1)[1]*dim(m1)[2])*100
m1[m1==999] <- NA

m1[paste0("FFMQ1.", rev_items)] <- 6-m1[paste0("FFMQ1.", rev_items)]

m1 <- data.frame("ID"=iq_c$ID.ffmq, m1)

#m1[which(is.na(rowSums(m1))), ]
# Delete 50 who's also missing ffmq data 
m1 <- m1[!m1$ID==50, ]

length(which(is.na(m1)))/(dim(m1)[1]*dim(m1)[2])*100

# For robustness check, casewise delete pts with any data missing 
m1_casewise_delete <- m1[-which(is.na(rowSums(m1))), ]

# Replace any NAs with the mean of the remainding items 
for (r in 1:nrow(m1)) {
  if (any(is.na(m1[r, 2:40]))) {
    this_row <- as.numeric(unlist(m1[r, 2:40])) # Exclude the ID 
    # Mean to replace NAs with 
    mean_non_na <- mean(this_row[which(!is.na(this_row))])
    this_row[is.na(as.numeric(unlist(this_row)))] <- mean_non_na
    # Put in df
    m1[r, 2:40] <- this_row 
  }    
}
#m1[which(is.na(rowSums(m1))), ]
#which(is.na(m1))

m1$aware_sum <- rowSums(m1[2:40][aware])
m1$nonjudge_sum <- rowSums(m1[2:40][non_judge])
m1$nonreact_sum <- rowSums(m1[2:40][non_react])
m1$observe_sum <- rowSums(m1[2:40][observe])

m1_casewise_delete$aware_sum <- rowSums(m1_casewise_delete[2:40][aware])
m1_casewise_delete$nonjudge_sum <- rowSums(m1_casewise_delete[2:40][non_judge])
m1_casewise_delete$nonreact_sum <- rowSums(m1_casewise_delete[2:40][non_react])
m1_casewise_delete$observe_sum <- rowSums(m1_casewise_delete[2:40][observe])

cor.test(m1$observe_sum, m1$aware_sum)
cor.test(m1$observe_sum, m1$nonjudge_sum)
cor.test(m1$observe_sum, m1$nonreact_sum)
cor.test(m1$aware_sum, m1$nonreact_sum)
```

```{r}
psych::alpha(m1[2:40][aware])
psych::alpha(m1[2:40][non_judge])
psych::alpha(m1[2:40][non_react])
psych::alpha(m1[2:40][observe])
```


```{r}
# Time 2 
# 2.24 % of final data missing 
m2 <- iq_c_m[grep("FFMQ2.", names(iq_c_m))]

sum(length(which(m2==999)), length(which(is.na(m2))))/(dim(m2)[1]*dim(m2)[2])*100
m2[m2==999] <- NA

m2[paste0("FFMQ2.", rev_items)] <- 6-m2[paste0("FFMQ2.", rev_items)]

m2 <- data.frame("ID"=iq_c$ID.ffmq, m2)
m2[which(is.na(rowSums(m2))), ]

ids_to_del <- c(56, 58)
# Delete 50 who's also missing ffmq data 
m2 <- m2 %>% filter(!ID %in% ids_to_del)

length(which(is.na(m2)))/(dim(m2)[1]*dim(m2)[2])*100

# For robustness check, casewise delete pts with any data missing 
m2_casewise_delete <- m2[-which(is.na(rowSums(m2))), ]

# Replace any NAs with the mean of the remainding items 
for (r in 1:nrow(m2)) {
  if (any(is.na(m2[r, 2:40]))) {
    this_row <- as.numeric(unlist(m2[r, 2:40])) # Exclude the ID 
    # Mean to replace NAs with 
    mean_non_na <- mean(this_row[which(!is.na(this_row))])
    this_row[is.na(as.numeric(unlist(this_row)))] <- mean_non_na
    # Put in df
    m2[r, 2:40] <- this_row 
  }    
}
#which(is.na(m2))

m2$aware_sum <- rowSums(m2[2:40][aware])
m2$nonjudge_sum <- rowSums(m2[2:40][non_judge])
m2$nonreact_sum <- rowSums(m2[2:40][non_react])
m2$observe_sum <- rowSums(m2[2:40][observe])

m2_casewise_delete$aware_sum <- rowSums(m2_casewise_delete[2:40][aware])
m2_casewise_delete$nonjudge_sum <- rowSums(m2_casewise_delete[2:40][non_judge])
m2_casewise_delete$nonreact_sum <- rowSums(m2_casewise_delete[2:40][non_react])
```
```{r}
psych::alpha(m2[2:40][aware])
psych::alpha(m2[2:40][non_judge])
psych::alpha(m2[2:40][non_react])
psych::alpha(m2[2:40][observe])
```


#### Model 1:  v ~ valence * session  

d1m is the MAP est df created above - rename d1me  
```{r}
d1me <- d1m
d1me
```

*--Valence: Baseline effects--*  


*Depression*  

```{r}
# Match the IDs in the qairre and map ests datasets  
dd1_red <- dd1 %>% 
  filter(ID %in% intersect(d1me$ID, dd1$ID)) %>% arrange(ID)

d1_red <- d1me %>% filter(ID %in% intersect(d1me$ID, dd1_red$ID)) %>% arrange(ID)

# Combine them 
if (all(d1_red$ID == dd1_red$ID)) fdf_depr <- data.frame(d1_red, dd1_red$dass_sum_score)
```


Examining how the machine learning procedure by Beevers et al. 19 JAP influences sparsity. Model draws on code included in the paper   

```{r}
var_names <- c(
      # 3, 5, 10, 13
      "no-positive-feelings", "couldnt-get-going", "easily-upset", "sad-and-depressed",
        # 16, 17, 21
        "lost-interest", "not-worth-much-as-person", "life-not-worthwhile", 
        # 24, 26, 31, 34, 37, 
        "no-enjoyment", "down-hearted-and-blue", "no-enthusiasm", "worthless", "no-hope", 
        #38, 42
        "life-meaningless", "no-initiative",
        "drift_regressor"
        )
```


Empirical 


```{r}
if (all(d1_red$ID == dd1_red$ID)) {
  t1_depr_combined <- data.frame(dd1_red, "v_val_ctr"=d1_red$v_val_ctr_)  
}
#cor.test(t1_depr_combined$v_val_ctr, t1_depr_combined$dass_sum_score)
cmat <- cor(t1_depr_combined %>% select(-c(ID, dass_sum_score)))
cmat
```

```{r}
just_rel_vars_emp <- cbind(t1_depr_combined[grep("DASS1", names(t1_depr_combined))], t1_depr_combined$v_val_ctr) %>% 
  setNames(var_names)
rf_out_emp <- beset_rf(drift_regressor ~ ., data=just_rel_vars_emp)
importance(rf_out_emp)
```


To examine the influence of sample size, we 

- Increase the n here - the other ones get more significnat, but the sparsity — their main conceptual point — is the 
Simulate multivariate normal based on true correlation matrix 





```{r}
sample_sizes <- c(100, 200, 500, 1e3, 5e3) 


for (ss in seq_along(sample_sizes)) {
  simulated_data <- rnorm_multi(n=sample_sizes[ss], vars=ncol(cmat), mu=0, sd=1, r=cmat)
  # Impose constraints on the depression items  
  depr_items <- round(simulated_data[1:14, 1:14], 0) 
  depr_items[depr_items < 0] <- 0
  depr_items[depr_items > 3] <- 3
  
  # Put them back in 
  simulated_data[1:14, 1:14] <- depr_items
  simulated_data <- simulated_data %>% setNames(var_names)
  
  rf_out <- beset_rf(drift_regressor ~ ., data=simulated_data)
  print(importance(rf_out))
}

```


```{r}
importance(rf_out)
```
 

Depression relationship 

```{r}
cor.test(fdf_depr$dd1_red.dass_sum_score, fdf_depr$v_val_ctr_)
```

Figure 4B, right  

```{r}
dvv <- ggplot(fdf_depr, aes(v_val_ctr_, dd1_red.dass_sum_score)) +
    geom_point(size=8, alpha=1, pch=21, fill="gray57") +
    #ggtitle(paste0( "r=-.30 p < .001 ")) +
    geom_smooth(method="lm", color="black") +
    ga + ap +
  geom_smooth(method="lm", color="black") +
    ga + theme(plot.title = element_text(margin=margin(b=-3), 
                    size=30, hjust=.15, vjust=.6)) + theme(axis.title = element_text(size=35), axis.text=element_text(size=35)) +
    ylab("baseline depression \n (via DASS)") + xlab("positive > negative \n drift rate estimates") +
  theme(plot.title = element_text(margin=margin(b=5), 
                    size=27, hjust=.15, vjust=.6)) + 
  geom_text(x=2.1, y=24.5, size=15, label="*****") + ylim(0, 37)

dvv
#ggsave("../paper/figs/depr_v-val.png", dvv, width=9, height=7)
```



*Mindfulness* 
```{r}
# Match the IDs in the qairre and map ests datasets  
m1_r <- m1 %>% 
  filter(ID %in% intersect(d1me$ID, m1$ID)) %>% arrange(ID)

d1_r <- d1me %>% filter(ID %in% intersect(d1me$ID, m1_r$ID)) %>% arrange(ID)

# Combine them 
if (all(d1_r$ID == m1_r$ID)) fdf <- data.frame(d1_r, 
                                               m1_r$aware_sum, m1_r$nonjudge_sum, m1_r$nonreact_sum, m1_r$observe_sum)
```



```{r}
summary(
  md_ddm_1 <- lm(v_val_ctr_ ~ 
           scale(m1_r.aware_sum)*scale(m1_r.nonjudge_sum) + 
           scale(m1_r.nonreact_sum) ,
   data=fdf)
)

car::vif(md_ddm_1)
```

```{r}
out <- summary(md_ddm_1)

reg_df <- data.table(c("aware", "non-judge", "non-react", "aware*non-judge"),
           out$coefficients[2:5, 1],
           out$coefficients[2:5, 2]) %>% setNames(c("coefficient", "estimate", "se"))

reg_df$coefficient <- factor(reg_df$coefficient, levels=c("aware", "non-judge", "non-react", "aware*non-judge"))

```

```{r}
mr <- ggplot(reg_df, aes(x=coefficient, y=estimate)) + 
  geom_errorbar(aes(ymin=estimate-se, ymax=estimate+se), size=2, width=0) +
  geom_point(fill="gray57", color="black", alpha=.8, size=8, pch=21) +
  geom_hline(yintercept=c(0), linetype="dotted") +
  theme(axis.text.x = element_text(angle=25, hjust=1)) +
  ga + ap + ylab("coefficients (±1 SEM)")  + xlab("") +
  ggtitle("Regression coefficients \n   of pos > neg drift rate ~ FFMQ mindfulness facets") + 
  theme(plot.title = element_text(margin=margin(b=5), 
                    size=27, hjust=.15, vjust=.6)) +
  geom_text(x=1, y=.17, size=15, label="*") + ylim(-.05, .18) +
  theme(axis.text = element_text(size=30),
               axis.title = element_text(size=30))

mr

#ggsave("../paper/figs/mindful_regr.png", mr, width=13, height=6)
```


Adding observe  
```{r}
summary(
  md_wobs_1 <- lm(v_val_ctr_ ~ 
           scale(m1_r.aware_sum)*scale(m1_r.nonjudge_sum) + 
           scale(m1_r.nonreact_sum) + scale(m1_r.observe_sum),
   data=fdf)
)

car::vif(md_wobs_1)
```

```{r}
summary(
  md_wobs_2 <- lm(v_val_ctr_ ~ 
           scale(m1_r.aware_sum)*scale(m1_r.nonjudge_sum) + 
           scale(m1_r.nonreact_sum) + scale(m1_r.observe_sum)*scale(m1_r.nonjudge_sum),
   data=fdf)
)

car::vif(md_wobs_2)
```


Robustness check: Confirm effects are same with casewise deletion  
```{r}
# Match the IDs in the qairre and map ests datasets  
m1_r_cd <- m1_casewise_delete %>% 
  filter(ID %in% intersect(d1me$ID, m1_casewise_delete$ID)) %>% arrange(ID)

d1_r_cd <- d1me %>% filter(ID %in% intersect(d1me$ID, m1_casewise_delete$ID)) %>% arrange(ID)

# Combine them 
if (all(d1_r_cd$ID == m1_r_cd$ID)) fdf_cd <- data.frame(d1_r_cd, 
                                                        m1_r_cd$aware_sum,
                                                        m1_r_cd$nonreact_sum,
                                                        m1_r_cd$nonjudge_sum)
```

```{r}
summary(
  md_d_1_cd <- lm(v_val_ctr_ ~ 
           scale(m1_r_cd.aware_sum)*scale(m1_r_cd.nonjudge_sum) + 
           scale(m1_r_cd.nonreact_sum),
   data=fdf_cd)
)

car::vif(md_d_1_cd)
```

```{r}
fd_r <- fdf %>% filter(ID %in% dd1_casewise_delete$ID) #%>% select(v_val_ctr_)

d1_r <- dd1_casewise_delete %>% filter(ID %in% fd_r$ID)
if (all(d1_r$ID==fd_r$ID)) cor.test(fd_r$v_val_ctr_, d1_r$dass_sum_score)
```

Mindfulness and depression in same  


```{r}
if (all(fdf$ID == fdf_depr$ID)) fdf <- data.frame(fdf, fdf_depr$dd1_red.dass_sum_score)
```


Depression relationship 

```{r}
# Sanity check
#cor.test(fdf$fdf_depr.dd1_red.dass_sum_score, fdf$v_val_ctr_)
cor.test(fdf$fdf_depr.dd1_red.dass_sum_score, fdf$m1_r.aware_sum)
```



```{r}
summary(
  all_m <- lm(v_val_ctr_ ~ 
           scale(m1_r.aware_sum)*scale(m1_r.nonjudge_sum) + 
           scale(m1_r.nonreact_sum) + scale(fdf_depr.dd1_red.dass_sum_score),
   data=fdf)
)

car::vif(all_m)
```

Univariate  

```{r}
cor.test(fdf$v_val_ctr_, fdf$m1_r.aware_sum)
```

## Change over time regressions   
Add dosage for time 

```{r}
big_df <- haven::read_sav("./../../data/raw_files/K23_Hitchcock_medpracticedata.sav")
big_df <- big_df %>% filter(ID %in% fdf$ID) %>% arrange(ID)
fdf_md <- fdf %>% filter(ID %in% big_df$ID) %>% arrange(ID)
if (all(fdf_md$ID==big_df$ID)) cor.test(fdf_md$v_val_ctr.sess_ctr_, big_df$TOTALminpwerwk.8wks)
```

We are just looking at FFMQ and depression at pre- and post-intervention and want to relate these to the SRET valence*time regressor. 
Had originally tried mixed-effects models to try to get time REs, but had convergence issues which make sense given low data per pt, so 
opting for simpler approach of change scores

```{r}
m1_shared <- m1 %>% filter(ID %in% m2$ID)
m2_shared <- m2 %>% filter(ID %in% m1$ID)

if (all(m1_shared$ID==m2_shared$ID)) {
  mindfulness_changes <- data.frame(m1_shared$ID,
             m2_shared$aware_sum-m1_shared$aware_sum,
             m2_shared$nonjudge_sum-m1_shared$nonjudge_sum,
             m2_shared$nonreact_sum-m1_shared$nonreact_sum,
             m2_shared$observe_sum-m1_shared$observe_sum) %>% 
    setNames(c("ID", "aware_change", "nj_change", "nr_change", "obs_change"))
}
hist(mindfulness_changes$nr_change)
hist(mindfulness_changes$nj_change)
hist(mindfulness_changes$aware_change)
hist(mindfulness_changes$obs_change)
```

```{r}
#hist(fdf$FFMQ_Aware_SUM_Q1)
#hist(fdf$FFMQ_Nonjudge_SUM_Q1)
a <- ggplot(fdf, aes(x=m1_r.aware_sum)) + 
  geom_histogram(bins=25, fill="gray38", color="black") + 
  xlim(5, 45) + ga + theme(axis.text=element_blank(), axis.ticks = element_blank()) + 
  ylab("") + xlab("") + ggtitle("Distributions of Mindfulness Facets at Baseline") + 
  theme(plot.title = element_text(margin=margin(b=-5), 
                    size=35, hjust=.15, vjust=.6)) +
  annotate("text", x = 40, y = 10, label = "Aware", size=10) 
  # annotate("text", x = 2.45, y = .85, label = TeX("yellow: $\\alpha$ < median"), size=13) 

b <- ggplot(fdf, aes(x=m1_r.nonjudge_sum)) + 
  geom_histogram(bins=25, fill="gray38", color="black") + 
  xlim(5, 45) + ga + ap + ylab("") + xlab("") +
  theme(axis.text.y=element_blank(), axis.ticks.y = element_blank())  +
  annotate("text", x = 35, y = 10, label = "Non-judgmental", size=10) 


comb_m <- a/b
comb_m

#ggsave("../paper/figs/mind_dist.png", comb_m, width=18, height=8)
```


```{r}
# mindfulness_changes$aware_change
# mindfulness_changes$aware_change
# range(mindfulness_changes$aware_change)
# range(mindfulness_changes$nj_change)
c <- ggplot(mindfulness_changes, aes(x=aware_change)) + 
  geom_histogram(bins=25, fill="gray75", color="black") + 
  ga + theme(axis.text=element_blank(), axis.ticks = element_blank()) + 
  ylab("") + xlab("") + ggtitle("Distributions of Change (Post - Pre) in Mindfulness Facets") + 
  theme(plot.title = element_text(margin=margin(b=-5), 
                    size=35, hjust=.15, vjust=.6)) +
  xlim(-14, 27) +
  annotate("text", x = 17, y = 11, label = "Aware", size=10) 
  # annotate("text", x = 2.45, y = .85, label = TeX("yellow: $\\alpha$ < median"), size=13) 

d <- ggplot(mindfulness_changes, aes(x=nj_change)) + 
  geom_histogram(bins=25, fill="gray75", color="black") + 
  ga + ap + ylab("") + xlab("") +
  theme(axis.text.y=element_blank(), axis.ticks.y = element_blank())  +
  xlim(-14, 27) +
  annotate("text", x = 17, y = 15, label = "Non-judgmental", size=10) 


comb_ch <- c/d
comb_ch
#ggsave("../paper/figs/mind_ch.png", comb_ch, width=18, height=8)
```

```{r}
d1me_shared <- d1me %>% filter(ID %in% mindfulness_changes$ID)
```

```{r}
d1me_shared
```

*Mindfulness change*  

```{r}
# Add drift rate change regressor
if (all(d1me_shared$ID==mindfulness_changes$ID)) {
  mindfulness_changes$dr_change <- d1me_shared$v_val_ctr.sess_ctr_  
}
```


```{r}
summary(
  change_1m <- 
    lm(dr_change ~ 
           scale(aware_change)*scale(nj_change) + 
           scale(nr_change),
   data=mindfulness_changes)
)

car::vif(change_1m)
```


```{r}
out_2 <- summary(change_1m)

out_2
reg_df_2 <- data.table(c("aware-change", "non-judge-change", "non-react-change", "aware*non-judge-change"),
           out_2$coefficients[2:5, 1],
           out_2$coefficients[2:5, 2]) %>% setNames(c("coefficient", "estimate", "se"))

reg_df_2$coefficient <- factor(reg_df_2$coefficient, levels=c("aware-change", "non-judge-change", "non-react-change", "aware*non-judge-change"))

```

```{r}
mr_2 <- ggplot(reg_df_2, aes(x=coefficient, y=estimate)) + 
  geom_errorbar(aes(ymin=estimate-se, ymax=estimate+se), color="gray57", size=2, width=0) +
  geom_point(fill="black", color="black", alpha=.3, size=8, pch=21) +
  geom_hline(yintercept=c(0), linetype="dotted") +
  theme(axis.text.x = element_text(angle=25, hjust=1)) +
  ga + ap + ylab("coefficients (±1 SEM)")  + xlab("") +
  ggtitle(paste0("Regression coefficients \n of pos > neg drift rate * time ~    FFMQ mindfulness facets")) + 
  theme(plot.title = element_text(margin=margin(b=5), 
                    size=27, hjust=.15, vjust=.6)) +
  geom_text(x=1, y=.092, size=15, label="**") + ylim(-.08, .10) +
  theme(axis.text = element_text(size=30),
               axis.title = element_text(size=30)) +
  scale_x_discrete(labels=c("aware-change" = "aware", "non-judge-change" = "non-judge",
                              "non-react-change" = "non-react", "aware*non-judge-change" = "aware*non-judge"))

mr_2

#ggsave("../paper/figs/mindful_regr_change.png", mr_2, width=13, height=6)
```

Robustness check: Add observe  change 

```{r}
summary(
  change_0m <- 
    lm(dr_change ~ 
           scale(aware_change)*scale(nj_change) + 
           scale(nr_change) + 
         scale(obs_change),
   data=mindfulness_changes)
)

car::vif(change_0m)
```

```{r}
summary(
  change_1m <- 
    lm(dr_change ~ 
           scale(aware_change)*scale(nj_change) + 
           scale(nr_change) + 
         scale(obs_change)*scale(nj_change),
   data=mindfulness_changes)
)

car::vif(change_1m)
```


Robustness checks: Holds in univariate model  
```{r}
cor.test(mindfulness_changes$dr_change, mindfulness_changes$aware_change)
```



*Depression change*  

```{r}
dd1_shared <- dd1 %>% filter(ID %in% dd2$ID)

dd2_shared <- dd2 %>% filter(ID %in% dd1$ID)



if (all(dd1_shared$ID==dd2_shared$ID)) {
  depr_changes <- data.frame(dd1_shared$ID,
             dd2_shared$dass_sum_score-dd1_shared$dass_sum_score) %>% 
    setNames(c("ID", "depr_change"))
}
hist(depr_changes$depr_change)

```


```{r}
d1me_shared_2 <- d1me %>% filter(ID %in% depr_changes$ID)
```

*Mindfulness change*  

```{r}
# Add drift rate change regressor
if (all(d1me_shared_2$ID==depr_changes$ID)) {
  depr_changes$dr_change <- d1me_shared_2$v_val_ctr.sess_ctr_  
}
```

```{r}
cor.test(depr_changes$dr_change, depr_changes$depr_change)
```


```{r}
dc_vvs <- ggplot(depr_changes, aes(dr_change, depr_change)) +
    geom_point(size=8, pch=21, alpha=.5, fill="gray57") +
    #ggtitle(paste0( "r=-.30 p < .001 ")) +
    #geom_smooth(method="lm", color="gray57") +
    ga + ap +
  geom_smooth(method="lm", color="gray57", alpha=.5) +
    ga + theme(plot.title = element_text(margin=margin(b=-3), 
                    size=30, hjust=.15, vjust=.6)) + theme(axis.title = element_text(size=35), axis.text=element_text(size=35)) +
    ylab("      depression \n (via DASS)") + xlab("positive > negative \n drift rate * time estimates") +
  theme(plot.title = element_text(margin=margin(b=5), 
                    size=27, hjust=.15, vjust=.6)) + xlim(-.45, .75) +
  geom_text(x=.55, y=0, size=15, label="*") #+ ylim(0, 37)

dc_vvs
#ggsave("../paper/figs/depr-change_v-val_sess.png", dc_vvs, width=9, height=7)
```
Robustness check: Adding depression as covariate in mindfulness model 
```{r}
if (all(mindfulness_changes$ID==depr_changes$ID)) {
  mindfulness_changes <- data.frame(mindfulness_changes, depr_changes$depr_change)
}

summary(
  change_2m <- 
    lm(dr_change ~ 
           scale(aware_change)*scale(nj_change) + 
           scale(nr_change) + scale(depr_changes.depr_change),
   data=mindfulness_changes)
)

cor.test(mindfulness_changes$depr_changes.depr_change, mindfulness_changes$aware_change)
```
Robustness check: Depression and DDM 
```{r}
cor.test(mindfulness_changes$dr_change_ddm, 
         mindfulness_changes$depr_changes.depr_change)
```



## Results: Split-half and test-retest reliability of model parameters   

```{r}
even_ddm <- read.csv("../../model_res/traces/DDM_split_half_even_wtrialwise_poutlier057074.csv")
odd_ddm <- read.csv("../../model_res/traces/DDM_split_half_odd__wtrialwise_poutlier052866.csv")
pre_t_d <- read.csv("./../../model_res/traces/DDM_test_retest_PRE_s-val_wtrialwise_poutlier059850.csv")
post_t_d <- read.csv("./../../model_res/traces/DDM_test_retest_POST_s-val_wtrialwise_poutlier055680.csv")
```

```{r}
even_emp <- read.csv("../../data/cleaned_files/s_bdf_even.csv")
odd_emp <- read.csv("../../data/cleaned_files/s_bdf_odd.csv")
# even_ddm <- read.csv("../../model_res/traces/DDM_split_half_even2558.csv")
# odd_ddm <- read.csv("../../model_res/traces/DDM_split_half_odd3780.csv")
```


```{r}
GetICCData <- function(vec1, vec2) {
  out <- psych::ICC(data.table(vec1, vec2))
data.table(out$results$ICC[2], out$results$`lower bound`[2], out$results$`upper bound`[2]) %>% 
  setNames(c("ICC", "95_ci_lb", "95_ci_ub"))
}
```

**Behavioral effects**  

*Split-half*  

```{r}
even_rt <- even_emp %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)

odd_rt <- odd_emp %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)

even_rt_n <- even_emp %>% filter(valence=="negative") %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)

odd_rt_n <- odd_emp %>% filter(valence=="negative") %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)

even_rt_p <- even_emp %>% filter(valence=="positive") %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)

odd_rt_p <- odd_emp %>% filter(valence=="positive") %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)
```



Calculate valence rt diffs  
```{r}
if (all(even_rt_n$subj_idx==odd_rt_n$subj_idx)) {
  even_v_rt_diff <- even_rt_p$rt - even_rt_n$rt 
  odd_v_rt_diff <- odd_rt_p$rt - odd_rt_n$rt 
  cor.test(even_v_rt_diff, odd_v_rt_diff)
  psych::ICC(data.table(even_v_rt_diff, odd_v_rt_diff))  
}
```

```{r}
even_pends <- even_emp %>%  filter( valence=="positive") %>% 
    group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)

even_nends <- even_emp %>%  filter(valence=="negative") %>% 
    group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)

even_pos_min_neg <- even_pends$response - even_nends$response

odd_pends <- odd_emp %>%  filter(valence=="positive") %>% 
    group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)

odd_nends <- odd_emp %>%  filter(valence=="negative") %>% 
    group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)

odd_pos_min_neg <- odd_pends$response - odd_nends$response
```


*Test-retest* 

```{r}
pre_rt <- bx_df %>%  filter(session=="Pre") %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)

post_rt <- bx_df %>%  filter(session=="Post") %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)

pre_rt_n <- bx_df %>%  filter(session=="Pre", valence=="negative") %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)

post_rt_n <- bx_df %>%  filter(session=="Post", valence=="negative") %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)


pre_rt_p <- bx_df %>%  filter(session=="Pre", valence=="positive") %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)

post_rt_p <- bx_df %>%  filter(session=="Post", valence=="positive") %>% 
    group_by(subj_idx) %>% summarize_at(c("rt"), mean, na.rm=TRUE)
```


```{r}
if (all(pre_rt$subj_idx == post_rt$subj_idx)) cor.test(pre_rt$rt, post_rt$rt); psych::ICC(data.table(pre_rt$rt, post_rt$rt))
if (all(pre_rt_n$subj_idx == post_rt_n$subj_idx)) cor.test(pre_rt_n$rt, post_rt_n$rt); psych::ICC(data.table(pre_rt_n$rt, post_rt_n$rt)); test <- "ok"
if (all(pre_rt_p$subj_idx == post_rt_p$subj_idx)) cor.test(pre_rt_p$rt, post_rt_p$rt); psych::ICC(data.table(pre_rt_p$rt, post_rt_p$rt)); test2 <- "ok"
```
Calculate valence rt diffs. A bit worse but even it's not bad  
```{r}
if (test=="ok" && test2=="ok") {
  pre_v_rt_diff <- pre_rt_p$rt - pre_rt_n$rt 
  post_v_rt_diff <- post_rt_p$rt - post_rt_n$rt 
  cor.test(pre_v_rt_diff, post_v_rt_diff)
  psych::ICC(data.table(pre_v_rt_diff, post_v_rt_diff))
} else {
  cat("Error with data alignment—see above cell.")
}
```


```{r}
pre_pends <- bx_df %>%  filter(session=="Pre", valence=="positive") %>% 
    group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)

pre_nends <- bx_df %>%  filter(session=="Pre", valence=="negative") %>% 
    group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)

pre_pos_min_neg <- pre_pends$response - pre_nends$response

post_pends <- bx_df %>%  filter(session=="Post", valence=="positive") %>% 
    group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)

post_nends <- bx_df %>%  filter(session=="Post", valence=="negative") %>% 
    group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)

post_pos_min_neg <- post_pends$response - post_nends$response

# post_pends <- bx_df %>%  filter(session=="Post", valence=="positive") %>% 
#     group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)
# 
# pre_pends <- bx_df %>%  filter(session=="Pre", valence=="positive") %>% 
#     group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)
# 
# post_pends <- bx_df %>%  filter(session=="Post", valence=="positive") %>% 
#     group_by(subj_idx) %>% summarize_at(c("response"), mean, na.rm=TRUE)
```

Following Hedge et al. 17 using ICC2


```Reliabilities were calculated using Intraclass Correlation Coefficients (ICC) using a two-way random effects model for absolute agreement. In the commonly cited Shrout and Fleiss (1979; see also McGraw & Wong, 1996) nomenclature, this corresponds to ICC (2,1). This form of the ICC is sensitive to differences between session means. In Supplementary Material A, we perform further analyses to account for potential outliers and distributional assumptions. The choice of statistic does not affect our conclusions. We report reliabilities separately for Studies 1 and 2 in the main text so that consistency across samples can be observed. We combine the studies in supplementary analyses.```


Split half  
```{r}
mf_ICCs_sh <- rbind(
  data.frame(GetICCData(even_rt$rt, odd_rt$rt), "mean \nreaction time") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(even_v_rt_diff, odd_v_rt_diff), "difference in \nreaction time by valence") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(even_nends$response, odd_nends$response), "proportion neg. endorsements") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(even_pends$response, odd_pends$response), "proportion pos. endorsements") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(even_pos_min_neg, odd_pos_min_neg), "ratio of positive:negative \n endorsements") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var"))
)
# mf_ICCs_sh$lb <- mf_ICCs_sh$ICC-mf_ICCs_sh$ci_lb
# mf_ICCs_sh$ub <- mf_ICCs_sh$ci_ub - mf_ICCs_sh$ICC

mf_ICCs_sh$type <- "split_half"
```
```{r}
mf_ICCs_sh
```



Test retest
```{r}
mf_ICCs_tr <- rbind(
  data.frame(GetICCData(pre_rt$rt, post_rt$rt), "mean \nreaction time") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(pre_v_rt_diff, post_v_rt_diff), "difference in \nreaction time by valence") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(pre_nends$response, post_nends$response), "proportion neg. endorsements") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(pre_pends$response, post_pends$response), "proportion pos. endorsements") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(pre_pos_min_neg, post_pos_min_neg), "ratio of positive:negative \n endorsements") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var"))
)
# mf_ICCs_tr$lb <- mf_ICCs_tr$ICC-mf_ICCs_tr$ci_lb
# mf_ICCs_tr$ub <- mf_ICCs_tr$ci_ub - mf_ICCs_tr$ICC

mf_ICCs_tr$type <- "test_retest"
```
```{r}
mf_ICCs_tr
```

```{r}
mf_iccs <- rbind(
  data.frame(mf_ICCs_sh, "label"="split-half"),
  data.frame(mf_ICCs_tr, "label"="test-retest")
)
```




**Model-based measures** 

*Split half*  

DDM  
```{r}
d_even_me <- GetMapEsts(GetSubjTraces(even_ddm), format="short")
d_odd_me <- GetMapEsts(GetSubjTraces(odd_ddm), format="short")

# Combine the datasets 
if (all(d_even_me$ID == d_odd_me$ID)) {
  c_me_d <- data.table(d_even_me, d_odd_me[2:ncol(d_odd_me)] %>% 
                       setNames(paste0(names(d_odd_me[2:ncol(d_odd_me)]), "odd")))    
}

mb_d_ICCs_sh <- rbind(
  data.frame(GetICCData(c_me_d$a_, c_me_d$a_odd), "threshold") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(c_me_d$t_, c_me_d$t_odd), "non-decision time") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  # Dummy var for this since not in DDM 
  # The one that generates convergence error.. also has lowest ICC 
  data.frame(GetICCData(InvLogit(c_me_d$z_), InvLogit(c_me_d$z_odd)), "starting point bias") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")), 
  data.frame(GetICCData(c_me_d$v_Intercept_, c_me_d$v_Intercept_odd), "drift rate - intercept") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(c_me_d$v_val_ctr_, c_me_d$v_val_ctr_odd), "drift rate - pos. vs. neg. \nregressor") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var"))
  #data.frame(GetICCData(data.table(even_rt$rt, odd_rt$rt), "RT mean") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var"))
)
```

```{r}
mb_d_ICCs
```

```{r}
# mb_d_ICCs_sh$lb <- mb_d_ICCs_sh$ICC-mb_d_ICCs_sh$ci_lb
# mb_d_ICCs_sh$ub <- mb_d_ICCs_sh$ci_ub-mb_d_ICCs_sh$ICC
```


```{r}
mb_d_ICCs_sh
```


*Test retest*  





DDM  
```{r}

#post_t_d <- read.csv("./../model_res/traces/DDM_test_retest_POST_m0_sret_v_val2062.csv") # Sub in the correct one 

d_pre_me <- GetMapEsts(GetSubjTraces(pre_t_d), format="short")
d_post_me <- GetMapEsts(GetSubjTraces(post_t_d), format="short")

# Combine the datasets 
if (all(d_pre_me$ID == d_post_me$ID)) {
  c_me_d <- data.table(d_pre_me, d_post_me[2:ncol(d_post_me)] %>% 
                       setNames(paste0(names(d_post_me[2:ncol(d_post_me)]), "post")))    
}


mb_d_ICCs <- rbind(
  data.frame(GetICCData(c_me_d$a_, c_me_d$a_post), "threshold") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(c_me_d$t_, c_me_d$t_post), "non-decision time") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  # Dummy var for this since not in DDM 
  # The one that generates convergence error.. also has lowest ICC 
  data.frame(GetICCData(InvLogit(c_me_d$z_), InvLogit(c_me_d$z_post)), "starting point bias") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")), 
  data.frame(GetICCData(c_me_d$v_Intercept_, c_me_d$v_Intercept_post), "drift rate - intercept") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var")),
  data.frame(GetICCData(c_me_d$v_val_ctr_, c_me_d$v_val_ctr_post), "drift rate - pos. vs. neg. \nregressor") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var"))
  #data.frame(GetICCData(data.table(pre_rt$rt, post_rt$rt), "RT mean") %>% setNames(c("ICC", "ci_lb", "ci_ub", "var"))
)

# mb_d_ICCs$lb <- mb_d_ICCs$ICC-mb_d_ICCs$ci_lb
# mb_d_ICCs$ub <- mb_d_ICCs$ci_ub-mb_d_ICCs$ICC
```


```{r}
ddm_iccs <- rbind(
  data.frame(mb_d_ICCs_sh, "label"="split-half"),
  data.frame(mb_d_ICCs, "label"="test-retest")
)
```


```{r}
mf_p <- ggplot(mf_iccs, aes(x=var, y=ICC, group=label, fill=label)) + 
  geom_errorbar(aes(ymin=ci_lb, ymax=ci_ub), position=position_dodge(width=.4), size=2, width=0) +
  geom_hline(yintercept=c(0, .25, .5, .75, 1)) +
  
  geom_point(alpha=.9, alpha=.9, size=8, pch=21, position=position_dodge(width=.4))  + 
  #geom_hline(yintercept = 0, linetype="dotted") +
  theme(axis.text.x = element_text(angle=25, hjust=1, size=16)) +
  ga + ap + ylab("") + ylim(-.1, 1) + xlab("") +
  #ggtitle("Model-derived measures") + 
  #theme(axis.text.y = element_blank(), axis.ticks.y=element_blank()) +
  theme(plot.title = element_text(margin=margin(b=-25), 
                    size=30, hjust=.15, vjust=.6)) + 
  scale_fill_manual(values=c("grey24", "gray65")) + lp #+
  # ggtitle("Behavioral measures") + 
  # theme(plot.title = element_text(margin=margin(b=-25), 
  #                   size=30, hjust=.15, vjust=.6))

mf_p
```

```{r}
mb_p <- ggplot(ddm_iccs, aes(x=var, y=ICC, group=label, fill=label)) + 
  geom_errorbar(aes(ymin=ci_lb, ymax=ci_ub), position=position_dodge(width=.4), size=2, width=0) +
  geom_hline(yintercept=c(0, .25, .5, .75, 1)) +
  
  geom_point(alpha=.9, alpha=.9, size=8, pch=21, position=position_dodge(width=.4))  + 
  #geom_hline(yintercept = 0, linetype="dotted") +
  theme(axis.text.x = element_text(angle=25, hjust=1, size=16)) +
  ga + ap + ylab("") + ylim(-.1, 1) + xlab("") +
  #ggtitle("Model-derived measures") + 
  #theme(axis.text.y = element_blank(), axis.ticks.y=element_blank()) +
  theme(plot.title = element_text(margin=margin(b=-25), 
                    size=30, hjust=.15, vjust=.6)) + 
  scale_fill_manual(values=c("tan1", "chocolate")) + lp
```

```{r, fig.height=8}
iccs_p <- mf_p / mb_p +
  plot_annotation(
  title = 'Stability of behavioral (top) and model-based (bottom) measures',
  subtitle='ICC point estimates and 95% CIs',
  theme = theme(plot.title = element_text(size = 25, hjust=.5),
                plot.subtitle = element_text(size = 20, hjust=.05))
)
iccs_p
```

```{r}
#ggsave("../paper/figs/icc_plot.png", iccs_p, width=11, height=10)
```

3.25 - Note that after R upgrade factor rule changes must have led to change in order on x-axis compared to version in figure in paper, but spot-checked to confirm all results were the same.  


